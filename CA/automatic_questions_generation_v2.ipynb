{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json, os, sys\n",
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(chunk, client, model=\"gpt-4o\"):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in news analysis and skilled at creating multiple-choice question answers that help to differentiate topics between articles. \n",
    "I have article summary and I need to generate unique, comprehensive multiple-choice questions based on these summaries.\n",
    "\n",
    "Your task is to generate questions that:\n",
    "- Help to differentiate topics between the articles.\n",
    "- Are common to all articles, not specific to any one article.\n",
    "- Are unique and not repeated in any other batch.\n",
    "- Include \"None of the above\" as one of the options for each question.\n",
    "\n",
    "Below is the format and example I need:\n",
    "\n",
    "1. What was done?\n",
    "   - A. The Chinese coast guard seized one of four food packs dropped by a Philippine military plane for Filipino navy personnel at a territorial outpost. After discovering the package contained food, they dumped it into the sea.\n",
    "   - B. Philippine soldiers were reported to have pointed guns at Chinese coast guard personnel during a resupply mission to the grounded Sierra Madre ship. The Chinese coast guard responded to the resupply operation, which included food drops, by observing armed Philippine soldiers on the shipâ€™s deck.\n",
    "   - C. Turkish Foreign Minister Hakan Fidan began a trip to China and expressed priorities to support Hamas against Israel and increase trade with China, without condemning the Uyghur genocide.\n",
    "   - D. The Philippines is collaborating with the United States and Japan to ensure the West Philippine Sea (WPS) remains free and safe amid tensions with Chinese maritime forces.\n",
    "   - E. None of the above\n",
    "\n",
    "Below is the questions that need multiple choice answers to be generated:\n",
    "\n",
    "1. What is the main theme of the article? \n",
    "2. What are the primary keywords or phrases in this article? \n",
    "3. What event or issue is the article centered around?\n",
    "4. Which region or country is primarily discussed in the article?\n",
    "5. During what time period is the event or issue discussed in the article occurring?\n",
    "6. Who is most affected by the issues discussed in the article?\n",
    "7. What type of sources does the article cite?\n",
    "\n",
    "Summaries: {chunk}\n",
    "\n",
    "Generate as many as you can unique questions based on the above summaries.\n",
    "\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model, messages=messages, temperature=0\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        return content\n",
    "    except Exception as e:  # if the model fails to return a response\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Sorry, error from GPT.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(input_file, output_file, client, model):\n",
    "    # Read the article summaries from the input JSON file\n",
    "    with open(input_file, \"r\") as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(articles)\n",
    "\n",
    "    # Extract summaries from the articles\n",
    "    article_summaries = [article[\"summary\"] for article in articles]\n",
    "    num_summaries = len(article_summaries)\n",
    "    batch_size = 200\n",
    "\n",
    "    responses = []\n",
    "\n",
    "    # Process the article summaries in batches of 100\n",
    "    for i in range(0, num_summaries, batch_size):\n",
    "        batch = article_summaries[i : i + batch_size]\n",
    "        chunk = json.dumps(batch)  # Convert the batch to a JSON string\n",
    "\n",
    "        questions = generate_questions(chunk, client, model=model)\n",
    "        responses.append(\n",
    "            {\n",
    "                \"batch_start\": i,\n",
    "                \"batch_end\": min(i + batch_size - 1, num_summaries - 1),\n",
    "                \"questions\": questions,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Write the generated questions to the output JSON file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"combined_Russo_Ukrainian_War.json\"\n",
    "output_file = \"output_questions_v2.json\"\n",
    "\n",
    "# Assuming you have your OpenAI API client initialized as `client`\n",
    "process_batches(input_file, output_file, client, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(batch_questions):\n",
    "    questions = {}\n",
    "    q_count = 1\n",
    "    for batch in batch_questions:\n",
    "        batch_qs = batch[\"questions\"].split(\"\\n\\n\")\n",
    "        for q in batch_qs:\n",
    "            match = re.match(\n",
    "                r\"\\d+\\. (.+?)\\n\\s+- A\\. (.+?)\\n\\s+- B\\. (.+?)\\n\\s+- C\\. (.+?)\\n\\s+- D\\. (.+?)\\n\\s+- E\\. (.+?)$\",\n",
    "                q.strip(),\n",
    "                re.DOTALL,\n",
    "            )\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                choices = {\n",
    "                    \"A\": match.group(2).strip(),\n",
    "                    \"B\": match.group(3).strip(),\n",
    "                    \"C\": match.group(4).strip(),\n",
    "                    \"D\": match.group(5).strip(),\n",
    "                    \"E\": match.group(6).strip(),\n",
    "                }\n",
    "                if question not in questions:\n",
    "                    questions[question] = choices\n",
    "    return questions\n",
    "\n",
    "\n",
    "def convert_to_json2_format(unique_questions):\n",
    "    formatted_questions = {}\n",
    "    q_num = 1\n",
    "    for question, choices in unique_questions.items():\n",
    "        formatted_questions[f\"Q{q_num}\"] = {\"question\": question, \"choices\": choices}\n",
    "        q_num += 1\n",
    "    return formatted_questions\n",
    "\n",
    "\n",
    "def process_json(input_json):\n",
    "    unique_questions = extract_questions(input_json)\n",
    "    return convert_to_json2_format(unique_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON 1\n",
    "with open(\"output_questions_v2.json\", \"r\") as f:\n",
    "    json1 = json.load(f)\n",
    "\n",
    "# Process JSON 1 to get JSON 2 format\n",
    "json2_format = process_json(json1)\n",
    "\n",
    "# Save the result to a new JSON file\n",
    "with open(\"formated_output_questions_v2.json\", \"w\") as f:\n",
    "    json.dump(json2_format, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
