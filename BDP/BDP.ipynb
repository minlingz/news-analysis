{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json, os, sys\n",
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(chunk, client, model=\"gpt-4o\"):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in news analysis and skilled at creating multiple-choice questions that help to differentiate between articles. \n",
    "I have some summaries of news articles and I need to generate unique, comprehensive multiple-choice questions based on these summaries.\n",
    "\n",
    "Your task is to generate questions that:\n",
    "- Focusing on Kenneth Burke’s Dramatistic Pentad, which is a tool for analyzing stories and narratives.\n",
    "- Help to differentiate between the articles.\n",
    "- Are common to all articles, not specific to any one article.\n",
    "- Are unique and not repeated in any other batch.\n",
    "- Include \"None of the above\" as one of the options for each question.\n",
    "\n",
    "Below is the format and example I need:\n",
    "\n",
    "1. What was done?\n",
    "   - A. The Chinese coast guard seized one of four food packs dropped by a Philippine military plane for Filipino navy personnel at a territorial outpost. After discovering the package contained food, they dumped it into the sea.\n",
    "   - B. Philippine soldiers were reported to have pointed guns at Chinese coast guard personnel during a resupply mission to the grounded Sierra Madre ship. The Chinese coast guard responded to the resupply operation, which included food drops, by observing armed Philippine soldiers on the ship’s deck.\n",
    "   - C. Turkish Foreign Minister Hakan Fidan began a trip to China and expressed priorities to support Hamas against Israel and increase trade with China, without condemning the Uyghur genocide.\n",
    "   - D. The Philippines is collaborating with the United States and Japan to ensure the West Philippine Sea (WPS) remains free and safe amid tensions with Chinese maritime forces.\n",
    "   - E. None of the above\n",
    "\n",
    "\n",
    "Please generate as many unique and comprehensive questions as possible based on the given article summaries below. \n",
    "Make sure each question is designed to highlight differences between the articles.\n",
    "\n",
    "Summaries: {chunk}\n",
    "\n",
    "Generate as many as you can unique questions based on the above summaries.\n",
    "\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model, messages=messages, temperature=0\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        return content\n",
    "    except Exception as e:  # if the model fails to return a response\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Sorry, error from GPT.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(input_file, output_file, client, model):\n",
    "    # Read the article summaries from the input JSON file\n",
    "    with open(input_file, \"r\") as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    # Shuffle the data\n",
    "    # random.shuffle(articles)\n",
    "\n",
    "    # Extract summaries from the articles\n",
    "    article_summaries = [article[\"summary\"] for article in articles]\n",
    "    article_summaries = (\n",
    "        article_summaries[200:300] + article_summaries[500:]\n",
    "    )  # Limit to 200 summaries\n",
    "    num_summaries = len(article_summaries)\n",
    "    batch_size = 200\n",
    "\n",
    "    responses = []\n",
    "\n",
    "    # Process the article summaries in batches of 100\n",
    "    for i in range(0, num_summaries, batch_size):\n",
    "        batch = article_summaries[i : i + batch_size]\n",
    "        chunk = json.dumps(batch)  # Convert the batch to a JSON string\n",
    "\n",
    "        questions = generate_questions(chunk, client, model=model)\n",
    "        responses.append(\n",
    "            {\n",
    "                \"batch_start\": i,\n",
    "                \"batch_end\": min(i + batch_size - 1, num_summaries - 1),\n",
    "                \"questions\": questions,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Write the generated questions to the output JSON file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../data/QnA_data/combined_summary.json\"\n",
    "output_file = \"output_questions.json\"\n",
    "\n",
    "# Assuming you have your OpenAI API client initialized as `client`\n",
    "process_batches(input_file, output_file, client, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(batch_questions):\n",
    "    questions = {}\n",
    "    q_count = 1\n",
    "    for batch in batch_questions:\n",
    "        batch_qs = batch[\"questions\"].split(\"\\n\\n\")\n",
    "        for q in batch_qs:\n",
    "            match = re.match(\n",
    "                r\"\\d+\\. (.+?)\\n\\s+- A\\. (.+?)\\n\\s+- B\\. (.+?)\\n\\s+- C\\. (.+?)\\n\\s+- D\\. (.+?)\\n\\s+- E\\. (.+?)$\",\n",
    "                q.strip(),\n",
    "                re.DOTALL,\n",
    "            )\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                choices = {\n",
    "                    \"A\": match.group(2).strip(),\n",
    "                    \"B\": match.group(3).strip(),\n",
    "                    \"C\": match.group(4).strip(),\n",
    "                    \"D\": match.group(5).strip(),\n",
    "                    \"E\": match.group(6).strip(),\n",
    "                }\n",
    "                if question not in questions:\n",
    "                    questions[question] = choices\n",
    "    return questions\n",
    "\n",
    "\n",
    "def convert_to_json2_format(unique_questions):\n",
    "    formatted_questions = {}\n",
    "    q_num = 1\n",
    "    for question, choices in unique_questions.items():\n",
    "        formatted_questions[f\"Q{q_num}\"] = {\"question\": question, \"choices\": choices}\n",
    "        q_num += 1\n",
    "    return formatted_questions\n",
    "\n",
    "\n",
    "def process_json(input_json):\n",
    "    unique_questions = extract_questions(input_json)\n",
    "    return convert_to_json2_format(unique_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON 1\n",
    "with open(\"output_questions.json\", \"r\") as f:\n",
    "    json1 = json.load(f)\n",
    "\n",
    "# Process JSON 1 to get JSON 2 format\n",
    "json2_format = process_json(json1)\n",
    "\n",
    "# Save the result to a new JSON file\n",
    "with open(\"formated_output_questions.json\", \"w\") as f:\n",
    "    json.dump(json2_format, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
